#
# Copyright (c) Lightly AG and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
from __future__ import annotations

import hashlib
import importlib
import logging
import os
import urllib.parse
from pathlib import Path
from typing import Any, Literal

import torch

from lightly_train._commands import common_helpers
from lightly_train._env import Env
from lightly_train._task_models.task_model import TaskModel
from lightly_train.types import PathLike

logger = logging.getLogger(__name__)

DOWNLOADABLE_MODEL_BASE_URL = (
    "https://lightly-train-checkpoints.s3.us-east-1.amazonaws.com"
)

LIGHTLY_TRAIN_PRETRAINED_MODEL = str

# How to add a new downloadable model:
# 1. Get hash of exported model file with `sha256sum best.pt`
# 2. Upload the exported model file to the S3 bucket and follow the naming scheme:
#    "<package>_<model_name>_[<task>]_<dataset>_[<resolution>]_<date>_<hash>.pt"
#    Example: dinov3_vitt16_ltdetr_coco_251205_1a4c20a1.pt
# 3. Add an entry to the DOWNLOADABLE_MODEL_URL_AND_HASH dictionary below including the
#    model name, file name, and hash.
DOWNLOADABLE_MODEL_URL_AND_HASH: dict[str, tuple[str, str]] = {
    #### Object Detection
    "dinov2/vits14-noreg-ltdetr-coco": (
        "dinov2_vits14_noreg_ltdetr_coco_251218_4e1f523d.pt",
        "4e1f523db68c94516ee5b35a91f24267657af474bea58b52a7f7e51ec2d8f717",
    ),
    "dinov2/vits14-ltdetr-dsp-coco": (
        "dinov2_vits14_ltdetr_dsp_coco_251218_fa435184.pt",
        "fa435184c775205469056f46456941ea271266ee522c656642853d061317f8ae",
    ),
    "dinov3/vitt16-ltdetr-coco": (
        "dinov3_vitt16_ltdetr_coco_251218_dfd34210.pt",
        "dfd34210a1a3375793d149a55d9b49e6e8b783458bdd4cd76fd28fa2d61dbb37",
    ),
    "dinov3/vitt16plus-ltdetr-coco": (
        "dinov3_vitt16plus_ltdetr_coco_251218_af499c82.pt",
        "af499c825436013098a77a028ff5cf08dbf31118f4d68b15eefa6fdd9635f5d2",
    ),
    "dinov3/vits16-ltdetr-coco": (
        "dinov3_vits16_ltdetr_coco_251218_4812416b.pt",
        "4812416b861a80f305889cf1408775044c8b05f1baf9be45cd4b1d0edd5d4532",
    ),
    "dinov3/convnext-tiny-ltdetr-coco": (
        "dinov3_convnext_tiny_ltdetr_coco_251218_35bbc4fb.pt",
        "35bbc4fbec3bb9fa113a33f1013abaab1952edf3335f98624b5914812d63d26c",
    ),
    "dinov3/convnext-small-ltdetr-coco": (
        "dinov3_convnext_small_ltdetr_coco_251218_8f7109ab.pt",
        "8f7109ab406aa92791e4e4ca6249ab9a863734795676c81b91dbd4cc4b1ef387",
    ),
    "dinov3/convnext-base-ltdetr-coco": (
        "dinov3_convnext_base_ltdetr_coco_251218_836adb6b.pt",
        "836adb6b5122665a24b6da3ee1720b9f3d0fc3c30cee44cfbd98dcb79fe0809a",
    ),
    "dinov3/convnext-large-ltdetr-coco": (
        "dinov3_convnext_large_ltdetr_coco_251218_03fe6750.pt",
        "03fe6750392daf3ecd32bbab3f144bd5c4d6cdc8bd75635f9e1c5e296e7dd8b0",
    ),
    "picodet-s-coco": (
        "picodet_s_coco_416_260109_ee0a9f46.pt",
        "ee0a9f4617c36222bdee77a7d87c9e041262af418679ae24143c5d284cb68511",
    ),
    "picodet-l-coco": (
        "picodet_l_coco_416_260109_7096f43c.pt",
        "7096f43c43e85d5bb46b6e96a4890d24894bb3dec497f5b826cdf2d3e8547226",
    ),
    #### Instance Segmentation
    "dinov3/vitt16-eomt-inst-coco": (  # 6x schedule
        "dinov3_vitt16_eomt_inst_coco_260109_45e0aff8.pt",
        "45e0aff8c5c8054a3240fcbc368b4e7f87e8066c1e100e3ef9d9c60c7d949a17",
    ),
    "dinov3/vitt16plus-eomt-inst-coco": (  # 6x schedule
        "dinov3_vitt16plus_eomt_inst_coco_260109_0e20aa05.pt",
        "0e20aa05ef15003d7d9462400d32ecc671e7a8d256ae061d42dd4f8978feb621",
    ),
    "dinov3/vits16-eomt-inst-coco": (
        "/dinov3_eomt/dinov3_vits16_eomt_inst_coco.pt",
        "b54dafb12d550958cc5c9818b061fba0d8b819423581d02080221d0199e1cc37",
    ),
    "dinov3/vitb16-eomt-inst-coco": (
        "/dinov3_eomt/dinov3_vitb16_eomt_inst_coco.pt",
        "a57b5e7afd5cd64422d74d400f30693f80f96fa63184960250fb0878afd3c7f6",
    ),
    "dinov3/vitl16-eomt-inst-coco": (
        "/dinov3_eomt/dinov3_vitl16_eomt_inst_coco.pt",
        "1aac5ac16dcbc1a12cc6f8d4541bea5e7940937a49f0b1dcea7394956b6e46e5",
    ),
    #### Panoptic Segmentation
    # Trained with 4x schedule (360k steps and the masking schedule of 90K steps)
    "dinov3/vitt16-eomt-panoptic-coco": (
        "dinov3_vitt16_eomt_panoptic_coco_260113_770c0a1f.pt",
        "770c0a1f024b9a78a6669d44968e2ab15b6d812839ce0c28732889ec5370ceea",
    ),
    "dinov3/vitt16plus-eomt-panoptic-coco": (
        "dinov3_vitt16plus_eomt_panoptic_coco_260113_25765911.pt",
        "25765911e4ebc6d735f385e8350a1c9924b4ccf08657d3868fbaa95ff4cc64e9",
    ),
    # Trained with 2x schedule (180k steps)
    "dinov3/vits16-eomt-panoptic-coco": (
        "dinov3_vits16_eomt_panoptic_coco_251219_89e8a64f.pt",
        "89e8a64fb601c509df76d09ed6ddb6789e080147cadcff9700cf5792dfc20167",
    ),
    # Trained with 2x schedule (180k steps)
    "dinov3/vitb16-eomt-panoptic-coco": (
        "dinov3_vitb16_eomt_panoptic_coco_251209_05948298.pt",
        "0594829822a23935079c35304f3bd1c7fede802114bc1a699780df693f2dea6c",
    ),
    "dinov3/vitl16-eomt-panoptic-coco": (
        "dinov3_vitl16_eomt_panoptic_coco_251209_e0c1e6ae.pt",
        "e0c1e6aeb245dbe6fd8735ffea48b81978b66b1a320533498de4375c18ad4368",
    ),
    "dinov3/vitl16-eomt-panoptic-coco-1280": (
        "dinov3_vitl16_eomt_panoptic_coco_1280_251209_3da0b210.pt",
        "3da0b21000bba3747bcb3e4ac4ee1e38641614022281f4b710d7442c643182f2",
    ),
    #### Semantic Segmentation
    "dinov3/vitt16-eomt-coco": (
        "dinov3_vitt16_eomt_coco_260106_104e563e.pt",
        "104e563ebcd8b7d2842db5f0cc6f8d0e67f1607a063ab818725e9af6f6fe7c27",
    ),
    "dinov3/vitt16plus-eomt-coco": (
        "dinov3_vitt16plus_eomt_coco_260106_68339a7d.pt",
        "68339a7d5baa0dd6fdd88660410939eb78fc8a8c9332145b9b8ac91a2291950b",
    ),
    "dinov3/vits16-eomt-coco": (
        "dinov3_vits16_eomt_coco_260105_11be50b5.pt",
        "11be50b578251c974b1fdb413c76e2cd7cfe1e154f6118556bd87477ea205d5a",
    ),
    "dinov3/vitb16-eomt-coco": (
        "dinov3_vitb16_eomt_coco_260105_92de5e05.pt",
        "92de5e0550f51647e201eef3537a35a8bba75b4e41323b9a7df3c54e6ab400b9",
    ),
    "dinov3/vitl16-eomt-coco": (
        "dinov3_vitl16_eomt_coco_260105_6169fdd8.pt",
        "6169fdd8edf7d4648c45c6aa1d09b9a4e917ba51dcbd36acf8fbf04a25d1e516",
    ),
    "dinov3/vitt32-eomt-coco": (
        "dinov3_vitt32_eomt_coco_260106_3ce75c95.pt",
        "3ce75c958aa0d31e3ac14d0bc1e0ca34ccb5b9ab5b141ec40c7f83c1950a2186",
    ),
    "dinov3/vitt32plus-eomt-coco": (
        "dinov3_vitt32plus_eomt_coco_260106_68e19609.pt",
        "68e196093301bc8a4e73005cebe1cccca75f5c14e58e732d1d9c555ea44e2088",
    ),
    "dinov3/vits32-eomt-coco": (
        "dinov3_vits32_eomt_coco_260106_06595b53.pt",
        "06595b53b0ee63032e8f7882a2d1e877c84b996c8313727a6694abf42e871d05",
    ),
    "dinov3/vitb32-eomt-coco": (
        "dinov3_vitb32_eomt_coco_260106_62cf509e.pt",
        "62cf509e156257347274837087592f27743ba51722c4949bec90688859cc6b6a",
    ),
    "dinov3/vitl32-eomt-coco": (
        "dinov3_vitl32_eomt_coco_260106_f51348fb.pt",
        "f51348fb4c794889ae35b8d9e2cfe383b42e09e975d2854f2e96fed155edd7d9",
    ),
    "dinov3/vits16-eomt-cityscapes": (
        "dinov3_eomt/lightlytrain_dinov3_eomt_vits16_cityscapes.pt",
        "ef7d54eac202bb0a6707fd7115b689a748d032037eccaa3a6891b57b83f18b7e",
    ),
    "dinov3/vitb16-eomt-cityscapes": (
        "dinov3_eomt/lightlytrain_dinov3_eomt_vitb16_cityscapes.pt",
        "e78e6b1f372ac15c860f64445d8265fd5e9d60271509e106a92b7162096c9560",
    ),
    "dinov3/vitl16-eomt-cityscapes": (
        "dinov3_eomt/lightlytrain_dinov3_eomt_vitl16_cityscapes.pt",
        "3f397e6ca0af4555adb1da9efa489b734e35fbeac15b4c18e408c63922b41f6c",
    ),
    "dinov3/vits16-eomt-ade20k": (
        "dinov3_eomt/lightlytrain_dinov3_eomt_vits16_autolabel_sun397.pt",
        "f9f002e5adff875e0a97a3b310c26fe5e10c26d69af4e830a4a67aa7dda330aa",
    ),
    "dinov3/vitb16-eomt-ade20k": (
        "dinov3_eomt/lightlytrain_dinov3_eomt_vitb16_autolabel_sun397.pt",
        "400f7a1b42a7b67babf253d6aade0be334173d70e7351a01159698ac2d2335ca",
    ),
    "dinov3/vitl16-eomt-ade20k": (
        "dinov3_eomt/lightlytrain_dinov3_eomt_vitl16_ade20k.pt",
        "eb31183c70edd4df8923cba54ce2eefa517ae328cf3caf0106d2795e34382f8f",
    ),
}


def load_model(
    model: PathLike,
    device: Literal["cpu", "cuda", "mps"] | torch.device | None = None,
) -> TaskModel:
    """Either load model from an exported model file (in .pt format) or a checkpoint file
    (in .ckpt format) or download it from the Lightly model repository.

    First check if `model` points to a valid file. If not and `model` is a `str` try to
    match that name to one of the models in the Lightly model repository and download it.
    Downloaded models are cached under the location specified by the environment variable
    `LIGHTLY_TRAIN_MODEL_CACHE_DIR`.

    Args:
        model:
            Either a path to the exported model/checkpoint file or the name of a model
            in the Lightly model repository.
        device:
            Device to load the model on. If None, the model will be loaded onto a GPU
            (`"cuda"` or `"mps"`) if available, and otherwise fall back to CPU.

    Returns:
        The loaded model.
    """
    device = _resolve_device(device)
    ckpt_path = download_checkpoint(checkpoint=model)
    ckpt = torch.load(ckpt_path, weights_only=False, map_location=device)
    model_instance = init_model_from_checkpoint(checkpoint=ckpt, device=device)
    return model_instance


def load_model_from_checkpoint(
    checkpoint: PathLike,
    device: Literal["cpu", "cuda", "mps"] | torch.device | None = None,
) -> TaskModel:
    """Deprecated. Use `load_model` instead."""
    return load_model(model=checkpoint, device=device)


def download_checkpoint(checkpoint: PathLike) -> Path:
    """Downloads a checkpoint and returns the local path to it.

    Supports checkpoints from:
    - Local file path
    - Predefined downloadable model names from our repository

    Returns:
        Path to the local checkpoint file.
    """
    ckpt_str = str(checkpoint)
    ckpt_path = Path(checkpoint).resolve()
    if ckpt_path.exists():
        # Local path
        local_ckpt_path = common_helpers.get_checkpoint_path(checkpoint=ckpt_path)
    elif ckpt_str in DOWNLOADABLE_MODEL_URL_AND_HASH:
        # Checkpoint name
        model_url, model_hash = DOWNLOADABLE_MODEL_URL_AND_HASH[ckpt_str]
        model_url = urllib.parse.urljoin(DOWNLOADABLE_MODEL_BASE_URL, model_url)
        download_dir = Env.LIGHTLY_TRAIN_MODEL_CACHE_DIR.value.expanduser().resolve()
        model_name = os.path.basename(urllib.parse.urlparse(model_url).path)
        local_ckpt_path = download_dir / model_name

        needs_download = True
        if not local_ckpt_path.is_file():
            logger.info(
                f"No cached checkpoint file found. Downloading from '{model_url}'..."
            )
        elif checkpoint_hash(local_ckpt_path) != model_hash:
            logger.info(
                "Cached checkpoint file found but hash is different. Downloading from "
                f"'{model_url}'..."
            )
        else:
            needs_download = False

        if needs_download:
            download_dir.mkdir(parents=True, exist_ok=True)
            torch.hub.download_url_to_file(url=model_url, dst=str(local_ckpt_path))
            logger.info(
                f"Downloaded checkpoint to '{local_ckpt_path}'. Hash: "
                f"{checkpoint_hash(local_ckpt_path)}"
            )
    else:
        raise ValueError(f"Unknown model name or checkpoint path: '{checkpoint}'")
    return local_ckpt_path


def init_model_from_checkpoint(
    checkpoint: dict[str, Any],
    device: Literal["cpu", "cuda", "mps"] | torch.device | None = None,
) -> TaskModel:
    # Import the model class dynamically
    module_path, class_name = checkpoint["model_class_path"].rsplit(".", 1)
    module = importlib.import_module(module_path)
    model_class = getattr(module, class_name)
    model_init_args = checkpoint["model_init_args"]
    model_init_args["load_weights"] = False

    # Create model instance
    model: TaskModel = model_class(**model_init_args)
    model = model.to(device)
    model.load_train_state_dict(state_dict=checkpoint["train_model"])
    model.eval()
    return model


def checkpoint_hash(path: Path) -> str:
    sha256_hash = hashlib.sha256()
    with open(path, "rb") as f:
        while block := f.read(4096):
            sha256_hash.update(block)
    return sha256_hash.hexdigest().lower()


def _resolve_device(device: str | torch.device | None) -> torch.device:
    """Resolve the device to load the model on."""
    if isinstance(device, torch.device):
        return device
    elif isinstance(device, str):
        return torch.device(device)
    elif device is None:
        if torch.cuda.is_available():
            # Return the default CUDA device if available.
            return torch.device("cuda")
        elif device is None and torch.backends.mps.is_available():
            # Return the default MPS device if available.
            return torch.device("mps")
        else:
            return torch.device("cpu")
    else:
        raise ValueError(
            f"Invalid device: {device}. Must be 'cpu', 'cuda', 'mps', a torch.device, or None."
        )
