{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LightlyTrain - Quick Start - Distillation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lightly-ai/lightly-train/blob/main/examples/notebooks/distillation.ipynb)\n",
    "\n",
    "This notebook demonstrates how to pretrain a model on **unlabeled data** with\n",
    "distillation. It follows the [quick start guide](https://docs.lightly.ai/train/stable/quick_start_distillation.html)\n",
    "from the documentation.\n",
    "\n",
    "Distillation is a special form of pretraining where a large, pretrained\n",
    "teacher model, like DINOv2 or DINOv3, is used to guide the training of a smaller student\n",
    "model. This is the ideal starting point if you want to improve performance of any\n",
    "model that is not already a large vision foundation model, like YOLO, ConvNet, or\n",
    "special transformer architectures.\n",
    "\n",
    "The quick start covers the following steps:\n",
    "1. Install LightlyTrain\n",
    "2. Prepare your unlabeled dataset\n",
    "3. Pretrain a model with distillation\n",
    "4. Fine-tune the pretrained model on a downstream task\n",
    "\n",
    "\n",
    "> **Important**: When running on Google Colab make sure to select a GPU runtime for faster processing. You can do this by going to `Runtime` > `Change runtime type` and selecting a GPU hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightly-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "> **Important**: LightlyTrain is officially supported on\n",
    "> - Linux: CPU or CUDA\n",
    "> - MacOS: CPU only\n",
    "> - Windows (experimental): CPU or CUDA\n",
    "> \n",
    "> We are planning to support MPS for MacOS.\n",
    "> \n",
    "> Check the [installation instructions](https://docs.lightly.ai/train/stable/installation.html) for more details on installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "You can use any image dataset for training. No labels are required, and the dataset can be structured in any way, including subdirectories. If you don't have a dataset at hand, you can download an example dataset from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/lightly-ai/coco128_unlabeled/releases/download/v0.0.1/coco128_unlabeled.zip && unzip -q coco128_unlabeled.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "See the [data guide](https://docs.lightly.ai/train/stable/pretrain_distill.html#train-data)\n",
    "for more information on supported data formats.\n",
    "\n",
    "In this example, the dataset looks like this:\n",
    "\n",
    "```\n",
    "coco128_unlabeled\n",
    "└── images\n",
    "    ├── 000000000009.jpg\n",
    "    ├── 000000000025.jpg\n",
    "    ├── ...\n",
    "    └── 000000000650.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Pretrain with Distillation\n",
    "\n",
    "Once the data is ready, you can pretrain the model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly_train\n",
    "\n",
    "# Pretrain the model\n",
    "lightly_train.pretrain(\n",
    "    out=\"out/my_experiment\",  # Output directory\n",
    "    data=\"coco128_unlabeled\",  # Directory with images\n",
    "    model=\"dinov3/vitt16\",  # Model to train\n",
    "    method=\"distillation\",  # Pretraining method\n",
    "    method_args={\n",
    "        \"teacher\": \"dinov3/vits16\"  # Teacher model for distillation\n",
    "    },\n",
    "    epochs=5,  # Small number of epochs for demonstration\n",
    "    batch_size=32,  # Small batch size for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "> **Note**: This is a minimal example for illustration purposes. In practice you would want to use a larger dataset (>=10'000 images), more epochs (>=100, ideally ~1000), and a larger batch size (>=128). For pretraining larger models than `dinov3/vitt16` we also recommend using a larger teacher model and setting `method=\"distillationv1\"`.\n",
    "\n",
    "> **Tip**: LightlyTrain supports many [popular models](https://docs.lightly.ai/train/stable/pretrain_distill/models/index.html) out of the box.\n",
    "\n",
    "This pretrains a tiny DINOv3 ViT model using images from `coco128_unlabeled`.\n",
    "All training logs, model exports, and checkpoints are saved to the output directory\n",
    "at `out/my_experiment`.\n",
    "\n",
    "Once the pretraining is complete, the `out/my_experiment` directory contains the\n",
    "following files:\n",
    "\n",
    "```text\n",
    "out/my_experiment\n",
    "├── checkpoints\n",
    "│   ├── epoch=03-step=123.ckpt          # Intermediate checkpoint\n",
    "│   └── last.ckpt                       # Last checkpoint\n",
    "├── events.out.tfevents.123.0           # Tensorboard logs\n",
    "├── exported_models\n",
    "|   └── exported_last.pt                # Final model exported\n",
    "├── metrics.jsonl                       # Training metrics\n",
    "└── train.log                           # Training logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "\n",
    "The final model is exported to `out/my_experiment/exported_models/exported_last.pt` in\n",
    "the default format of the used library. It can directly be used for\n",
    "fine-tuning. See [export format](https://docs.lightly.ai/train/stable/pretrain_distill/export.html#format) for more information on how to export\n",
    "models to other formats or on how to export intermediate checkpoints.\n",
    "\n",
    "While the trained model has already learned good representations of the images, it\n",
    "cannot yet make any predictions for tasks such as classification, detection, or\n",
    "segmentation. To solve these tasks, the model needs to be fine-tuned on a labeled\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Fine-Tune\n",
    "\n",
    "Now the model is ready for fine-tuning! You can use your favorite library for this step.\n",
    "We'll use LightlyTrain's built-in fine-tuning for object detection as an example.\n",
    "\n",
    "### Prepare Labeled Data\n",
    "\n",
    "A labeled dataset is required for fine-tuning. You can download an example dataset\n",
    "from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/lightly-ai/coco128_yolo/releases/download/v0.0.1/coco128_yolo.zip && unzip -q coco128_yolo.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The dataset looks like this after the download completes:\n",
    "```\n",
    "coco128_yolo\n",
    "├── images\n",
    "│   ├── train2017\n",
    "│   │   ├── 000000000009.jpg\n",
    "│   │   ├── 000000000025.jpg\n",
    "│   │   ├── ...\n",
    "│   │   └── 000000000650.jpg\n",
    "│   └── val2017\n",
    "│       ├── 000000000139.jpg\n",
    "│       ├── 000000000285.jpg\n",
    "│       ├── ...\n",
    "│       └── 000000013201.jpg\n",
    "└── labels\n",
    "    ├── train2017\n",
    "    │   ├── 000000000009.txt\n",
    "    │   ├── 000000000025.txt\n",
    "    │   ├── ...\n",
    "    │   └── 000000000659.txt\n",
    "    └── val2017\n",
    "        ├── 000000000139.txt\n",
    "        ├── 000000000285.txt\n",
    "        ├── ...\n",
    "        └── 000000013201.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Fine-Tune the Pretrained Model\n",
    "\n",
    "Once the dataset is ready, you can fine-tune the pretrained model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly_train\n",
    "\n",
    "lightly_train.train_object_detection(\n",
    "    out=\"out/my_finetune_experiment\",\n",
    "    model=\"dinov3/vitt16-ltdetr\",\n",
    "    model_args={\n",
    "        # Load the pretrained weights.\n",
    "        \"backbone_weights\": \"out/my_experiment/exported_models/exported_last.pt\",\n",
    "    },\n",
    "    steps=100,  # Small number of steps for demonstration, default is 90_000.\n",
    "    batch_size=4,  # Small batch size for demonstration, default is 16.\n",
    "    data={\n",
    "        \"path\": \"coco128_yolo\",\n",
    "        \"train\": \"images/train2017\",\n",
    "        \"val\": \"images/val2017\",\n",
    "        \"names\": {\n",
    "            0: \"person\",\n",
    "            1: \"bicycle\",\n",
    "            2: \"car\",\n",
    "            3: \"motorcycle\",\n",
    "            4: \"airplane\",\n",
    "            5: \"bus\",\n",
    "            6: \"train\",\n",
    "            7: \"truck\",\n",
    "            8: \"boat\",\n",
    "            9: \"traffic light\",\n",
    "            10: \"fire hydrant\",\n",
    "            11: \"stop sign\",\n",
    "            12: \"parking meter\",\n",
    "            13: \"bench\",\n",
    "            14: \"bird\",\n",
    "            15: \"cat\",\n",
    "            16: \"dog\",\n",
    "            17: \"horse\",\n",
    "            18: \"sheep\",\n",
    "            19: \"cow\",\n",
    "            20: \"elephant\",\n",
    "            21: \"bear\",\n",
    "            22: \"zebra\",\n",
    "            23: \"giraffe\",\n",
    "            24: \"backpack\",\n",
    "            25: \"umbrella\",\n",
    "            26: \"handbag\",\n",
    "            27: \"tie\",\n",
    "            28: \"suitcase\",\n",
    "            29: \"frisbee\",\n",
    "            30: \"skis\",\n",
    "            31: \"snowboard\",\n",
    "            32: \"sports ball\",\n",
    "            33: \"kite\",\n",
    "            34: \"baseball bat\",\n",
    "            35: \"baseball glove\",\n",
    "            36: \"skateboard\",\n",
    "            37: \"surfboard\",\n",
    "            38: \"tennis racket\",\n",
    "            39: \"bottle\",\n",
    "            40: \"wine glass\",\n",
    "            41: \"cup\",\n",
    "            42: \"fork\",\n",
    "            43: \"knife\",\n",
    "            44: \"spoon\",\n",
    "            45: \"bowl\",\n",
    "            46: \"banana\",\n",
    "            47: \"apple\",\n",
    "            48: \"sandwich\",\n",
    "            49: \"orange\",\n",
    "            50: \"broccoli\",\n",
    "            51: \"carrot\",\n",
    "            52: \"hot dog\",\n",
    "            53: \"pizza\",\n",
    "            54: \"donut\",\n",
    "            55: \"cake\",\n",
    "            56: \"chair\",\n",
    "            57: \"couch\",\n",
    "            58: \"potted plant\",\n",
    "            59: \"bed\",\n",
    "            60: \"dining table\",\n",
    "            61: \"toilet\",\n",
    "            62: \"tv\",\n",
    "            63: \"laptop\",\n",
    "            64: \"mouse\",\n",
    "            65: \"remote\",\n",
    "            66: \"keyboard\",\n",
    "            67: \"cell phone\",\n",
    "            68: \"microwave\",\n",
    "            69: \"oven\",\n",
    "            70: \"toaster\",\n",
    "            71: \"sink\",\n",
    "            72: \"refrigerator\",\n",
    "            73: \"book\",\n",
    "            74: \"clock\",\n",
    "            75: \"vase\",\n",
    "            76: \"scissors\",\n",
    "            77: \"teddy bear\",\n",
    "            78: \"hair drier\",\n",
    "            79: \"toothbrush\",\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This will load the pretrained model from `out/my_experiment/exported_models/exported_last.pt`\n",
    "and fine-tune it on a subset of the labeled COCO dataset for 100 steps.\n",
    "\n",
    "Congratulations! You've just trained and fine-tuned a model using Lightly Train!\n",
    "\n",
    "> **Note**: This is a minimal example for illustration purposes. In practice you would want to use more steps (>=10,000), and a larger batch size (16).\n",
    "\n",
    "> **Note**: LightlyTrain already provides object detection models that were pretrained\n",
    "  and fine-tuned on COCO so you don't have to do all these steps yourself. Check out the\n",
    "  [Object Detection Quick Start](https://docs.lightly.ai/train/stable/quick_start_detection.html)\n",
    "  on how to use these models directly.  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "If you want to learn more about fine-tuning and how to use the fine-tuned model for\n",
    "inference, head over to the [Object Detection Quick Start](https://docs.lightly.ai/train/stable/quick_start_detection.html).\n",
    "\n",
    "If you want to learn more about distillation and how to pretrain any model with it,\n",
    "head over to the [Distillation Guide](https://docs.lightly.ai/train/stable/pretrain_distill.html).\n",
    "\n",
    "If you want to learn how to pretrain foundation models, check out the [DINOv2 Pretraining page](https://docs.lightly.ai/train/stable/pretrain_dinov2.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Embed\n",
    "\n",
    "Instead of fine-tuning the model, you can also use it to generate image embeddings. This is useful for clustering, retrieval, or visualization tasks. The `embed` command generates embeddings for all images in a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightly_train\n",
    "\n",
    "lightly_train.embed(\n",
    "    out=\"my_embeddings.pth\",  # Exported embeddings\n",
    "    checkpoint=\"out/my_experiment/checkpoints/last.ckpt\",  # LightlyTrain checkpoint\n",
    "    data=\"coco128_unlabeled\",  # Directory with images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The embeddings are saved to `my_embeddings.pth`. Let's load them and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embeddings = torch.load(\"my_embeddings.pth\")\n",
    "print(\"First five filenames:\")\n",
    "print(embeddings[\"filenames\"][:5])  # Print first five filenames\n",
    "print(\"\\nEmbedding tensor shape:\")\n",
    "print(\n",
    "    embeddings[\"embeddings\"].shape\n",
    ")  # Tensor with embeddings with shape (num_images, embedding_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
